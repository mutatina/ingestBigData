import subprocess,csv,urllib2,json,time

def createPropertyfile(tableInfoDictionary,varFile,minload,maxload):
"""
Name:createPropertyfile
Description:This function creates .properties file using the CSV and TXT files	given along with  min and max load-dates if available.
Parameters:config text file path, min and max load Dates, infomation about the mysql table
"""
	
	#get required info from varFile

	readFile= open(varFile,'r')
	file_str= readFile.read()
	readFile.close()
	
	hqlScriptStart= file_str.find('hqlScriptFiles')+15
	hqlScriptEnd = hqlScriptStart+file_str[hqlScriptStart:].find('\n')
	hqlScriptFiles =file_str[hqlScriptStart:hqlScriptEnd]
	
	hqlScriptDict= eval(hqlScriptFiles)
	hqlScriptFile=""
	if eval(tableInfoDictionary['partitioned_tbl']):
		print minload,maxload
		if minload and maxload:   #If dates have been provided
			print 'Dates have been provided'
			hqlScriptFile= hqlScriptDict['hqlFilePartRange']
		else:	
			print 'No dates provided ,using script for loading ALL partitioned table'
			hqlScriptFile=	hqlScriptDict['hqlFilePart']
			minload='NA'
			maxload='NA'

	else: 
		print "Non partitioned table detected going to use script for not partitioned table"
		hqlScriptFile=hqlScriptDict['hqlFileNonPart']
		minload='NA'
		maxload='NA'

	fileStrWithoutHql=file_str.replace(file_str[hqlScriptStart-15:hqlScriptEnd],'').strip()
	

	#preparing to add the elements in the file
	requiredString=fileStrWithoutHql +'\n'
	requiredString=requiredString + "min_load_date="+ minload +'\n'
	requiredString=requiredString + "max_load_date="+ maxload +'\n'
	requiredString=requiredString + "hqlScriptFile="+ hqlScriptFile +'\n'
	
	

        outputFileString =tableInfoDictionary['target_table']+".properties"
	outputFile= open(outputFileString,'w')
	outputFile.write(requiredString)
	#writing everything from the csv file

	for key,value in tableInfoDictionary.iteritems():
       		outputFile.write(key +'='+ value +'\n')	
	outputFile.close()
	print 'properties file generated: ',outputFileString
	return


# Given table name this function will run the two hql files(stg & parquet) for the table in the hive shell

def createTableInHive(table_name):
"""
Name:createTableInHive
Description:This function creates and executes Hive commands to create tables in Hive using the HQLs.	
Parameters: table_name
"""

	stageHqlfileLocation = '/home/cloudera/hive/create_' + table_name + '_stg.hql'
	parquetHqlfileLocation ='/home/cloudera/hive/create_' + table_name + '_parquet.hql'

	fileList=[stageHqlfileLocation,parquetHqlfileLocation]
	for _file in fileList:
		hiveCommand = "hive -f " + _file
		
		try:
			subprocess.check_call(hiveCommand,shell=True)
		except subprocess.CalledProcessError, e:
			print ("ERROR :Could not create the Table with "+_file)
			
		else:
			print "Table created succesfully"

	return

def runIngestWorkflow(table_name,jobsDict):
"""
Name:runIngestWorkflow
Description:This function runs the oozie command  for the given table  and reports the results in a success/fail dictionary given.
Parameters:Name of the table to be ingested and a dictionary to record results.
"""

	propertiesFileLocationPath = table_name + ".properties"
	
	oozieCommand =	"oozie job -oozie http://localhost:11000/oozie -config "+propertiesFileLocationPath +" -run"
	
		
	p=subprocess.Popen(oozieCommand,shell=True,stderr=subprocess.PIPE,stdout=subprocess.PIPE)
	
	error_output= p.stderr.readline()
	success_output = p.stdout.readline()
		
	if error_output:
			
		print ("ERROR Could not run the oozie workflow for table :"+table_name)
		print('Details:',error_output)
		jobsDict['fail'][table_name]=error_output

	if success_output:
		oozieJobID = success_output[success_output.find(':')+1:success_output.find('\n')].strip()	
		print ("Ozzie job succesfully submitted for table : "+table_name)
		print ('oozieJobID ',oozieJobID)
		jobsDict['success'][table_name]=oozieJobID

	return








def getTables(csvFile):
"""
Name:getTables
Description:Given the csv file location this function returns a list of tables found in the file.	
Parameters:csv file location path
"""
	tableList=[]
	with open(csvFile) as csvfile:
		reader = csv.DictReader(csvfile)
		for row in reader:
			tableList.append(row)  
	return tableList


def getJobStatus(jobsDict):
"""
Name:getJobStatus
Description: This function checks and report the status of succesfully submited oozie jobs in the dictionary provided, and reports the final 	        results back to the dictionary.	
Parameters:Dictionary of success and failed jobs
"""
	URL_start = 'http://localhost:11000/oozie/v1/job/'
	URL_finish = '?show=info&timezone=GMT'
	
	numberOfJobs=len(jobsDict['success'])
	jobsCompleted = 0

	while (jobsCompleted < numberOfJobs):
		for table,status in jobsDict['success'].iteritems():
			if status not in ['FAILED','KILLED','TIMEDOUT','SUCCEEDED']:
				
							
				Target_job =status
				FULL_URL = URL_start + Target_job + URL_finish
				req = urllib2.Request(FULL_URL)
				response = urllib2.urlopen(req)
				outputDict = json.loads(response.read())

				if (outputDict['status'] =='SUCCEEDED'):				
					
					jobsDict['success'][table]='SUCCEEDED'
					jobsCompleted = jobsCompleted + 1
					print('Current status for table:',table, ' is ',outputDict['status'])
					
					

				elif outputDict['status'] in ['FAILED','KILLED','TIMEDOUT']:
					print('Current status for table:',table, ' is ',outputDict['status'])
					jobsCompleted = jobsCompleted + 1		
					jobsDict['fail'][table]=outputDict['status']
					jobsDict['success'][table]=outputDict['status']
					
					
				else:	
					print 'Current status for table: '+ table  +' is ' + outputDict['status']
				
					time.sleep(5)
						

	return



def runAggregationWorkflow(jobsDict):
"""
Name:runAggregationWorkflow
Description:	This function basically checks if any ingestion workflow jobs have failed and if not then it runs the aggregation workflow.
Parameters:Dictionary of success and failed jobs
"""
	if jobsDict['fail']: 
		print "Can not run Aggregation  workflow , there exists at least one table that failed to move."
		return
	else:
		print "All submitted jobs succedded, running the aggregation workflow now"
		#Call aggregation workflow here
		return





def cleanPartitionCol(table_name,partition_col):
"""
Name:cleanPartitionCol
Description: This handy utitlity  cleans/removes the extra partition-column line in HQL files:	
Parameters:table name and the partition column to be removed in the hql file
"""
	
	parquetHqlfileLocation ='/home/cloudera/hive/create_' + table_name + '_parquet.hql'

	fileList=[parquetHqlfileLocation]
	for _file in fileList:
		f = open(_file)
		file_str = f.read() 
		f.close()
		file_str_temp=file_str.lower()
		
		extraLineStarts= file_str_temp.find(partition_col.lower()) - 1 
		extraLineEnds= extraLineStarts + file_str_temp[extraLineStarts:].find('\n')  + 1
		
		extraLine =file_str_temp[extraLineStarts:extraLineEnds]
		print "Removing: "+ extraLine + 'from file '+ _file
		file_str=file_str.replace(file_str[extraLineStarts:extraLineEnds],"")
	
		f = open(_file, 'w') 
		f.write(file_str)
		f.close()



